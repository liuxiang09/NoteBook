# 《InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning》

> - 在持续学习中，模型应具有在旧任务上保持其性能的能力（稳定性）和持续适应新任务的能力（可塑性）。
> - 参数高效微调（PEFT）在持续学习中大受欢迎，它通过冻结预训练权重并注入少量可学习参数以适应下游任务。
> - 尽管现有的PEFT方法比不基于PEFT的方法性能更好，但是大多数方法没有考虑如何消除新任务对旧任务的干扰，阻碍了模型在稳定性和可塑性之间的权衡。
> - 文章提出了一个新的基于PEFT的方法，叫做无干扰低秩适应（InfLoRA），用于持续学习。
> - InfLoRA注入少量参数来重新参数化预训练的权重，并表明微调这些注入的参数等同于微调子空间内的预训练权重。
> - 此外，InfLoRA设计了这个子空间，以消除新任务对旧任务的干扰。
> - 实验结果表明，InfLoRA在多个数据集上优于现有的最先进的持续学习方法。

[TOC]

## 1. Introduction

- 持续学习经常考虑两种场景：**任务增量场景**和**类增量场景**。
- 任务增量场景允许模型在推理过程中获取任务身份，相反，类增量场景不允许模型在推理过程中获取任务身份，而是使模型能够区分所有任务的所有类。
- 当学习新任务时，现有的持续学习方法要么重新使用先前学习的参数来适应新的任务，要么先随机扩展一些参数再适应新任务（比如额外的权重矩阵，LoRA分支）。

**文章的贡献如下：**

- InfLoRA注入了少量的参数来重参数化预训练权重，并表明微调这些注入参数等价于在一个子空间内微调预训练权重。
- InfLoRA设计了一个子空间来消除新任务对老任务的干扰，使模型在稳定性和可塑性上做出更好的权衡。
- 实验结果表明InfLoRA在多个数据集上的性能优于现有的最先进的持续学习方法。

## 2. Related Work and Preliminaries

### 2.1 Related Work

- 参数高效微调（PEFT）：参数高效微调减少了完整微调中低效率的学习方法，即微调预训练模型中的所有参数来学习下游任务。
- **Adapter**方法在模型的各层之间插入小型可训练模块（适配器），这些模块在微调过程中被训练以适应新任务，而原始模型的参数保持冻结。这种方法通过增加少量参数，实现对模型行为的调整。
- **Prompt-tuning**方法在输入序列的嵌入层添加可训练的提示向量（软提示），这些向量在训练过程中被优化，以引导模型产生特定任务的输出。Prompt Tuning 仅调整输入层的提示向量，保持模型其余部分的参数不变。
- **Prefix Tuning**：在输入序列的开头添加可训练的前缀，这些前缀作为虚拟标记，影响模型的生成过程。与 Prompt Tuning 类似，Prefix Tuning 通过在输入中添加前缀来控制生成结果的格式和结构，但它不仅影响输入层，还影响模型的各个层次。
- 与 Adapter、Prompt Tuning 和 Prefix Tuning 等方法相比，LoRA 的主要区别在于其直接对模型内部的权重矩阵进行低秩分解和调整，而不是通过添加额外的模块或输入前缀来实现微调。 这种方法可以在保持模型性能的同时，减少微调所需的参数量和计算成本。
- 实验证明完全微调预训练模型的方法是无效的，但这些PEFT方法都没有考虑到消除新任务对老任务的干扰，并在稳定性和可塑性之前取得良好的平衡。

### 2.2 Preliminaries

- **LoRA**方法假设当模型对下游任务进行完全微调时，参数的变化位于**低秩空间**。

- Low-Rank Adaptation (LoRA) [16] 是最流行的参数高效微调（PEFT）方法之一。它假设当模型在下游任务上进行完整微调时，参数的变化处于一个低秩空间中。

- 具体来说，对于一个输入维度为 $d_I$、输出维度为$d_O$的线性层，其权重矩阵 $\boldsymbol{W}^{d_{O} \times d_{I}}$。LoRA 通过增加一个由两个矩阵组成的分支来重新参数化预训练权重 \(W\)，这两个矩阵分别为：

  - $A \in \mathbb{R}^{d_O \times r}$：升维矩阵  
  - $B \in \mathbb{R}^{r \times d_I}$：降维矩阵  

  通常，秩 \(r\) 的大小远小于输入维度 \($d_I$\) 和输出维度 \($d_O$\)。最终，LoRA 将此线性层中的前向传播公式修改为：

  $$
  e = W h + ABh
  $$

  其中，$h$ 和 $e$ 分别表示此层的输入和输出。

  LoRA 初始化矩阵 $A$ 为零矩阵，并用高斯分布对矩阵 $B$ 进行初始化。在学习下游任务的过程中，LoRA **冻结预训练权重 $W$**，仅微调矩阵 $A$ 和 $B$ 的参数。

## 3. Methodology

![image-20250113191518015](../Typora_images/image-20250113191518015.png)

#### InfLoRA 与 LoRA 的主要区别

1. **针对增量学习任务的设计**：
   - **LoRA**：主要用于单任务或静态环境下的微调，所有参数 $A$和 $B$都是同时训练的，没有任务的增量更新。
   - **InfLoRA**：针对 **类增量学习场景（class-incremental scenario）**，在每个任务 $t$中，引入新的增维矩阵 $A_t$和降维矩阵 $B_t$，从而扩展模型的能力，同时保持之前任务的知识。
2. **矩阵 $ A_t $的初始化方式**：
   - LoRA 和 InfLoRA 均将增维矩阵 $A_t$初始化为 **0**，目的是避免模型在初始阶段对权重的扰动。
   - 初始化为 **0** 的好处是，在初始状态下，模型的行为完全由原始权重 $W$决定，降低不稳定性。
3. **矩阵 $ B_t$的处理方式**：
   - **LoRA**：直接通过**高斯分布**初始化矩阵$ B$，并在整个微调过程中同时训练 $A$和 $B$。
   - **InfLoRA**：在学习第 $t$个任务之前，**设计降维矩阵 $B_t$**。这意味着 $B_t$的初始化不是随机的，而是通过特定的设计（例如考虑梯度空间的关系，如前图所示）来确定。这种设计旨在确保 $B_t$在新任务中能够有效地捕获新知识，同时与旧任务的梯度空间兼容。
4. **参数冻结策略**：
   - LoRA 不会冻结任何 $A$或 $B$，所有任务的适配器都同时参与训练。
   - InfLoRA 在学习新任务时，会**冻结旧任务的所有 **$A_j$、$B_j$$（j < t）$，以及预训练权重 $W$。这样可以避免新任务对旧任务知识的干扰，解决灾难性遗忘问题。

#### 针对 $A_t$ 和 $B_t$设计的独特之处

1. **增维矩阵** $A_t$：
   - **初始化为 0**，确保任务学习的过程是从预训练模型的基础上逐步建立起来的，而不会受到随机初始化的干扰。
   - 在增量学习过程中，**仅对当前任务的 $A_t$进行训练**，从而保证之前任务的知识保持不变。
2. **降维矩阵 $B_t$**：
   - LoRA 的 $B$是高斯随机初始化的，而 InfLoRA 通过**梯度空间设计**，确保 $B_t $能有效表示新任务的特性，同时与旧任务保持兼容性。
   - $B_t$在任务开始前被设计，而不是通过随机初始化学习，这种“先设计后优化”的方式大幅降低了新任务学习的干扰。

#### 子空间的概念

1. **降维矩阵 $ B_t $的作用**：
   - $ B_t $将输入空间 $\mathbb{R}^{d_I}$ 投影到一个低维子空间 $\mathbb{R}^r$。
   - $ B_t $的行张成了这个子空间的基底，因此任何通过 $ B_t $的线性组合都受限于这个子空间。
2. **增维矩阵 $ A_t $的作用**：
   -  $ A_t $是可训练的矩阵，它负责将这个低维子空间的特征重新映射回输出空间 $\mathbb{R}^{d_o}$。
   - $A_t B_t$ 的结果实际上限制了模型只能在 $ B_t $的子空间内调整权重。

#### 训练  $ A_t $等价于在 $ B_t $的子空间内训练 $W$

1. **训练  $ A_t $**：
   - 当  $ A_t $被训练时，实际上是在学习如何对 $ B_t $的行空间进行组合，以适应新的任务。
     - $ B_t $的设计决定了权重更新的方向和范围，因此 $A_t B_t$ 是受 $ B_t $的行张成子空间限制的。
2. **对比直接训练 $W$**：
   - 如果直接训练 $W$，模型可以自由地调整所有的权重。
   - 而在 InfLoRA 中，通过训练  $ A_t $，权重更新被限制在 $ B_t $的行张成的低秩子空间中，这相当于对 $W$进行了一种受限优化。
3. **数学等价性**：
   - 调整  $ A_t $相当于找到一个合适的线性组合，使得 $A_t B_t$ 最好地近似所需的权重变化。
   - 因此，训练  $ A_t $实质上是对 $W$进行优化，但优化范围仅限于 $ B_t $所定义的子空间。

### 3.1. Relationship between InfLoRA and FineTuning the Pre-Trained Weight

#### 证明的问题（Proposition 1）

- 问题描述：在学习第 $t $个任务时（使用公式 (1) 的前向传播方式），微调 $A_t$是否等价于在由 $B_t$的行张成的子空间 $\text{span}(b_1^t, \dots, b_r^t)$中微调 $W$？
  - $b_i^t$ 是降维矩阵 $B_t$ 的第 $i$ 行向量。
  - 目标是展示微调 $A_t$ 对 $W_t$ 的影响实际上受限于 $B_t$ 定义的低秩子空间。

#### 证明过程

##### 1. 微调$$W$$的梯度推导

根据链式法则，权重矩阵$W$的梯度为：

$$
\frac{\partial \mathcal{L}}{\partial W} = \frac{\partial \mathcal{L}}{\partial e} \cdot \frac{\partial e}{\partial W} = \frac{\partial \mathcal{L}}{\partial e} \cdot h^T
$$

其中：
- $$\mathcal{L}$$ 是损失函数。
- $$e$$ 是线性层的输出。
- $$h$$ 是输入向量。

因此，微调 $$W$$ 的变化为：

$$
\Delta W = -\alpha \cdot \frac{\partial \mathcal{L}}{\partial W} = -\alpha \cdot \frac{\partial \mathcal{L}}{\partial e} \cdot h^T
$$

这里 $$\alpha$$ 是学习率。

---

##### 2. 微调 $$A_t$$ 的梯度推导

同理，当调整 $$A_t$$ 时，其梯度可以表示为：

$$
\frac{\partial \mathcal{L}}{\partial A_t} = \frac{\partial \mathcal{L}}{\partial e} \cdot \frac{\partial e}{\partial A_t} = \frac{\partial \mathcal{L}}{\partial e} \cdot h^T \cdot B_t^T
$$

因此，微调 $$A_t$$ 的变化为：

$$
\Delta A_t = -\alpha \cdot \frac{\partial \mathcal{L}}{\partial A_t}
$$

然后，使用公式 (1) 中 $$W_t = W_{t-1} + A_t B_t$$，可以得到 $$W_t$$ 的变化：

$$
\Delta_{A_t} W_t = [W_{t-1} + (A_t + \Delta A_t) B_t] - (W_{t-1} + A_t B_t)
$$

化简得到：

$$
\Delta_{A_t} W_t = \Delta A_t \cdot B_t = -\alpha \cdot \frac{\partial \mathcal{L}}{\partial e} \cdot h^T \cdot B_t^T \cdot B_t
$$

---

##### 3. 比较 $$W$$ 和 $$A_t$$ 对 $$W_t$$ 的影响

根据第 1 步，调整 $$W$$ 对 $$W_t$$ 的变化为：

$$
\Delta_W W_t = -\alpha \cdot \frac{\partial \mathcal{L}}{\partial e} \cdot h^T
$$

而调整 $$A_t$$ 对 $$W_t$$ 的变化为：

$$
\Delta_{A_t} W_t = \Delta_W W_t \cdot B_t^T \cdot B_t
$$

从公式可以看出：
- 调整 $$A_t$$ 对 $$W_t$$ 的变化是通过矩阵 $$B_t^T B_t$$ 对 $$\Delta_W W_t$$ 进行投影。
- 矩阵 $$B_t^T B_t$$ 是一个投影矩阵，将 $$\Delta_W W_t$$ 的更新限制在 $$B_t$$ 的行张成的子空间内。

---

##### 4. 结论

$$
\Delta_{A_t} W_t = \Delta_W W_t \cdot B_t^T \cdot B_t
$$

因此，微调 $$A_t$$ 等价于将预训练权重 $$W$$ 的更新限制在 $$B_t$$ 的行张成的子空间内。命题得证。

### 3.2. Eliminating the Interference of the New Task on the Old Tasks

> 首先介绍InfLoRA希望让子空间具有的特征，有了这些特征，InfLoRA就可以消除旧任务对新任务的干扰，使其能在稳定性和可塑性之间做出更好地权衡。
>
> 然后介绍如何设计降维矩阵$B_t$使得它的子空间 $\text{span}\{b_1^t \dots b_r^t\}$ 能够具有这些特征。

#### 3.2.1 Desired Characteristics

##### 期望的特征：

1. **新任务更新与旧任务梯度的正交性**：
   - 期望新任务更新的方向与旧任务梯度的空间正交。
   - 根据 Proposition 1，当 $B_t$的行张成的子空间 $\text{span}\{b_1^t, \dots, b_r^t\}$ 与旧任务梯度的空间正交时，InfLoRA 的更新对旧任务的干扰最小。
   - 这种正交性确保新任务的学习不会对旧任务的性能造成干扰，从而避免灾难性遗忘。

2. **子空间包含新任务梯度空间**：
   - 除了与旧任务梯度正交外，还期望 $B_t$ 的子空间包含新任务梯度所在的低秩子空间。
   - 这确保模型可以有效地捕获新任务的特征，同时保持模型的高效性和可塑性。
   - **基于现有研究（如 [19]），微调过程中权重的增量通常是冗余的，因此新任务的梯度通常位于低维子空间内。**

#### 3.2.2 Designing Dimensionality Reduction Matrix

> InfLoRA 通过设计降维矩阵 $B_t$，旨在实现对新任务和旧任务之间的干扰最小化，同时在稳定性（旧任务性能）和可塑性（新任务适应能力）之间取得良好平衡。以下是设计 $B_t$ 的关键点和具体方法：

##### 1. 目标特性

为了消除新任务对旧任务的干扰，并保持模型适应新任务的能力，$B_t$ 需要具备以下特性：

- **正交性**：$B_t$ 的行向量所定义的子空间 $\text{span}\{b_1^t, \dots, b_r^t\}$ 与旧任务的梯度空间 $\mathcal{M}_t$ 正交，从而保证新任务的更新不会影响旧任务的性能。
- **包含性**：$B_t$ 的子空间 $\text{span}\{b_1^t, \dots, b_r^t\}$ 应该位于新任务梯度空间 $\mathcal{N}_t$ 中，以确保模型能够有效学习新任务。

##### 2. 设计方法

1. **梯度空间的近似**：
   - 新任务的梯度空间 $\mathcal{N}_t$ 使用其输入矩阵 $H_t = [h_1^t, \dots, h_r^t]$ 的列向量来近似，假设每列代表新任务的一个输入向量。
   - 旧任务的梯度空间 $\mathcal{M}_t$ 由于无法直接获取旧任务数据，通过 **DualGPM** 技术预先保存其正交基。
2. **交集空间计算**：
   - 当 ${M}_t$ 已知时，**InfLoRA** 对新任务输入矩阵 $H_t$ 执行操作 $\hat{H}_t = H_t - {M}_t {M}_t^T H_t$，去除 $H_t$ 中与 ${M}_t$ 重叠的成分。
   - 如果维护的是 ${M}_t$ 的正交补 ${M}_t^\perp$，则执行 $\hat{H}_t = {M}_t^\perp ({M}_t^\perp)^T H_t$。
3. **主成分分析（PCA）提取**：
   - 为了从形状为 $(\hat{H}_t)^T \in \mathbb{R}^{n \times d_I}$ 的矩阵中选取降维矩阵 $B_t$，对其进行奇异值分解 (SVD)：$(\hat{H}_t)^T = V_t \Sigma_t U_t$
   - 最终 $B_t$ 由 $U_t$ 的前 $r$ 个奇异值对应的行向量构成，即 $B_t = (U_t)_r$。

##### 3. 矩阵设计的优势

- **避免干扰**：通过让 $\text{span}\{b_1^t, \dots, b_r^t\}$ 与旧任务的梯度空间正交，有效消除了新任务对旧任务的干扰。
- **保持可塑性**：将 $B_t$ 限制在新任务的梯度空间 $\mathcal{N}_t$ 内，确保模型能够专注于新任务。

##### 4. 辅助技术

InfLoRA 借助 DualGPM 来维护旧任务梯度信息，并通过优化正交补 $\mathcal{M}_t^\perp$ 的维度，减缓空间受限对新任务学习能力的负面影响。

### 3.3. Whole Process of InfLoRA

> **3.3. Whole Process of InfLoRA** 详细描述了 **InfLoRA 在持续学习中的完整训练流程**，并引入了 **局部 CE 损失、参数冻结+合并、梯度存储等关键优化策略**。这些设计共同作用，使得 InfLoRA **既能减少新任务对旧任务的干扰（稳定性），又能高效学习新任务（可塑性）**，从而在持续学习任务中取得更好的平衡。

#### **1. 训练流程概述**

论文用 **算法 1**（Algorithm 1）详细描述了 **InfLoRA 在持续学习中的完整执行流程**，大致分为以下几个阶段：

1. **任务初始化（第 $t$ 个任务到来）**
   - 设计 $B_t$（降维矩阵），具体方法参见 **3.2. Eliminating the Interference of the New Task on the Old Tasks**。
   - 为新任务 **扩展一个新的 LoRA-like 分支**，包含参数 $A_t$（升维矩阵）和 $B_t$（降维矩阵）。
2. **训练第 $t$ 个任务**
   - 以 **小批量（mini-batch）** 方式采样数据 $B_t$。
   - 计算损失 $L(f_{\Theta}(B_t))$ 并 **更新参数 $A_t$**（但 $W$ 和所有旧任务分支均冻结）。
3. **存储梯度信息**
   - 训练完新任务后，**使用 DualGPM 方法** 存储该任务的梯度信息，以便后续任务学习时减少干扰。

------

#### **2. 关键优化策略**

##### **(1) 使用局部交叉熵（Local Cross-Entropy, CE）损失**

- 论文选择 局部 CE 损失（local CE loss）进行训练，而不是全局 CE 损失：
  $$
  L(D_t) = \frac{1}{|D_t|} \sum_{(x,y) \in D_t} L_{ce}(\text{mask}(h_{\Phi}(f_{\Theta}(x))), y)
  $$
  其中：

  - $f_{\Theta}(\cdot)$：预训练 ViT 模型（Vision Transformer）。
  - $h_{\Phi}(\cdot)$：分类器。
  - **mask(·)**：仅保留当前任务的类别 logits，防止未见过的类别影响模型优化。
  - $L_{ce}$：标准交叉熵损失。

**为什么使用局部 CE？**

- 现有研究表明，**局部 CE 通常比全局 CE 在持续学习任务中效果更好**，可以减少旧任务类别对新任务的影响，提高模型稳定性。

##### **(2) 冻结旧任务分支，合并参数减少存储**

- 训练完 $t$任务后：
  - **冻结 $A_t, B_t$**，不再调整已学任务的分支，防止干扰旧任务。
  - 合并 LoRA 分支，减少存储开销：
    - 初始时，模型权重为 $W$。
    - 学完第 1 个任务后，将第 1 个分支合并： $W_1 = W + A_1 B_1$
    - 继续训练时，存储的主权重会逐步累加： $W_t = W_{t-1} + A_t B_t$
    - **这样，每次训练只需存储一个新任务的** $A_t, B_t$，存储需求 **始终为** $(d_I + d_O)r$，不会随任务增多而指数级增长。

------

#### **3. 主要优势**

- **减少灾难性遗忘**（Catastrophic Forgetting）
  由于每个任务都分配独立的 LoRA-like 分支，旧任务参数不会被新任务覆盖，避免遗忘问题。
- **存储开销小**
  只存储最新任务的 $A_t, B_t$，不需要存储所有旧任务的独立参数，提升了模型的可扩展性。
- **适用于任务未知的 Class-Incremental Learning 场景**
  由于 InfLoRA **不会依赖任务标识（task identity）**，适用于任务数量未知、类别动态增长的持续学习场景。

## 4. Experiments

### 4.1. Experimental Settings

#### **1. 数据集与评估指标**

##### **(1) 数据集**

论文使用了 **三个常见的持续学习（continual learning）数据集**：

- **ImageNet-R**（200 类）：从 ImageNet 选取 200 个类别，并进行风格化处理，以增加数据的多样性和挑战性。
  - 分为 **5、10、20 个任务**，每个任务包含 **40、20、10 个类别**。
- **CIFAR-100**（100 类）：经典图像分类数据集，划分为 **10 个任务，每个任务包含 10 个类别**。
- **DomainNet**（345 类）：一个跨域数据集，划分为 **5 个任务，每个任务包含 69 个类别**。

##### **(2) 评估指标**

论文使用了 **两种衡量持续学习效果的标准指标**：

1. 最终准确率（Final Accuracy, $ACC_T$）：

   - 训练完所有任务后，模型在所有已学任务上的平均准确率。

2. 平均准确率（Averaged Accuracy, $ACC_T$）：

   - 计算 所有任务的平均准确率：
     $$
     ACC_T = \frac{1}{T} \sum_{i=1}^{T}
     $$
     

     其中，$ACC_i$是模型在训练到第 $i$个任务时，在 所有已学任务上的平均准确率：
     $$
     ACC_i = \frac{1}{i} \sum_{j=1}^{i} a_{i,j}
     $$

     - $a_{i,j}$代表模型在学习第 $i$ 个任务后，对第 $j$ 个任务的测试准确率。

------

#### **2. 对比方法（Baselines）**

论文与 **多种最新的持续学习方法进行对比**，分为两类：

##### **(1) 持续学习方法**

- **L2P**（Learn to Prompt）
- **DualPrompt**
- **CODA-P**（Continual Decomposed Attention-Based Prompting）
- **C-LoRA**（Continual LoRA）
- **LAE**（Learning Accumulation Ensemble with LoRA）

##### **(2) 其他基线**

- **Joint**：所有任务一起训练（上界）。
- **Sequential**：顺序训练所有任务，不做特殊处理（下界）。

------

#### **3. 训练设置**

##### **(1) 预训练模型**

- 使用 **ViT-B/16**（Vision Transformer，Base 版本，Patch Size 16）。
- 该模型 **在 ImageNet-21K 上进行监督预训练**，然后用于持续学习。

##### **(2) 训练细节**

- **优化器**：Adam（$\beta_1 = 0.9, \beta_2 = 0.999$）
- 训练轮数：
  - **ImageNet-R**：50 轮
  - **CIFAR-100**：20 轮
  - **DomainNet**：5 轮
- **批量大小**：128
- 参数插入位置：
  - LoRA-like 结构 **插入 ViT Transformer 的 Key 和 Value 层**。
  - 与 LoRA 相关的方法（如 InfLoRA）**在所有 Transformer 层插入**，而 DualPrompt、CODA-P 等方法 **仅在底部 5 层插入**。

------

#### **4. 主要结论**

- **数据集设置**：涵盖了小型（CIFAR-100）、中型（DomainNet）和大型（ImageNet-R）数据集，验证方法的通用性。
- **评估指标**：使用了 **最终准确率和平均准确率**，更全面衡量持续学习的效果。
- **对比方法全面**：包括多种 **基于 Prompt 和 LoRA 的最新方法**，以及 **无持续学习优化的上下界方法**。
- **实验设置合理**：采用 **主流 ViT 预训练模型**，并与现有工作保持一致，保证公平性。

### 4.2. Experimental Results

![image-20250209151935853](../Typora_images/image-20250209151935853.png)

![image-20250209152154121](../Typora_images/image-20250209152154121.png)

#### **1. 主要实验结果**

##### **(1) ImageNet-R 结果分析**

- 从 Table 1（见前面的表格分析）可以看出，InfLoRA 在所有任务数量（5、10、20 任务）上均表现最佳：
  - **5 任务：77.52%（最终准确率），82.01%（任务完成时的准确率）**
  - **10 任务：75.65%（最终准确率），80.82%（任务完成时的准确率）**
  - **20 任务：71.01%（最终准确率），77.28%（任务完成时的准确率）**
- **相较于其他方法，InfLoRA 在所有任务规模下均取得最高分**，证明其 **在大规模持续学习任务中仍然有效**。

##### **(2) CIFAR-100 结果分析**

- **InfLoRA 仍然优于其他方法，特别是在任务数量增加时（10 任务设定下），其准确率比 C-LoRA、LAE 高 1-2%**。
- **相比于 Prompt 相关方法（如 L2P, DualPrompt, CODA-P），InfLoRA 在 CIFAR-100 上的优势更加明显**，这可能是因为 **LoRA 结构更适合在小型数据集上进行低秩适配**，避免过拟合。

##### **(3) DomainNet 结果分析**

- 在更具挑战性的 DomainNet 数据集上，**InfLoRA 依然在所有任务数量下表现最佳**。
- **相比于 C-LoRA 和 LAE，InfLoRA 在 10 任务设定下的准确率提高了 2.5%-3%**，进一步证明了 **其在跨域任务中的稳定性**。

------

#### **2. 关键发现**

##### **(1) InfLoRA 具有最强的抗灾难性遗忘能力**

- **Sequential（顺序训练）方法表现最差**，在 20 任务设定下，最终准确率仅有 **34.62%**，表明 **灾难性遗忘严重**。
- **InfLoRA 在 20 任务情况下仍保持 71.01% 的最终准确率，比 Sequential 高出 36.39%！**
- **C-LoRA 和 LAE 也有效减少了灾难性遗忘，但仍比 InfLoRA 低 2%-3%**。

##### **(2) LoRA 方法比 Prompt 方法更稳定**

- L2P, DualPrompt, CODA-P 的表现普遍低于 LoRA 方法（C-LoRA, LAE, InfLoRA）：
  - 在 10 任务和 20 任务情况下，Prompt 方法的准确率下降较快，**因为 Prompt 仅修改少量参数，无法充分适应新任务**。
  - **InfLoRA 通过低秩适配（Low-Rank Adaptation），在降低计算成本的同时提高了适应能力**，比 C-LoRA 和 LAE 更具优势。

##### **(3) InfLoRA 能够在不同数据集和任务规模下保持最优**

- **无论是小型数据集（CIFAR-100）、中型数据集（DomainNet），还是大型数据集（ImageNet-R），InfLoRA 都能取得最高准确率**。
- 这说明 **InfLoRA 具备良好的泛化能力，能够适应不同的数据分布和任务结构**。

------

#### **3. 结论**

- **InfLoRA 在所有实验设定下均取得最优表现，显著减少灾难性遗忘，并保持高准确率。**
- **相比于 C-LoRA 和 LAE，InfLoRA 在 10 任务和 20 任务设定下的优势更加明显，说明其适用于长期的持续学习任务。**
- **相比于 Prompt 方法（L2P, DualPrompt, CODA-P），LoRA 方法（尤其是 InfLoRA）表现更稳定，适应性更强，能更好地应对任务转移。**
- **最终结论：InfLoRA 是当前最优的持续学习方法之一，在不同任务规模和数据集上均表现卓越，是减少灾难性遗忘的最佳选择。**

