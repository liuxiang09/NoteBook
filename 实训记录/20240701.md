每天写实习日志

完成训前评测

### 一、虚拟机安装及环境搭建

VMware

Virtualbox

```bash
一、环境搭建
1、linux环境搭建
   #01虚拟机安装
     #Vmware
     #Virtualbox
   #02设置外部工具访问
     vi /etc/ssh/sshd_config
     #修改密码认证
     PasswordAuthentication yes   
     PermitRootLogin yes
     #修改密钥认证
     PubkeyAuthentication yes
   #03设置静态地址
     vi /etc/sysconfig/network-scripts/ifcfg-eth0
     NM_CONTROLLED=yes
     DEVICE=eth0
     ONBOOT=yes 
     BOOTPROTO=static #设置为静态static，动态则为dhcp
     IPADDR=192.168.245.130 #静态ip地址
     NETMASK=255.225.225.0 #子网掩码
     GATEWAY=192.168.245.2 #网关地址
     DNS1=114.114.114.114 #配置DNS
     DNS2=8.8.8.8
   #04修改映射名称
     vi /etc/hosts
	 192.168.56.131 hadoop001
   #05设置免密登录
	 执行 ssh-keygen -t rsa 回车到底
	 https://blog.csdn.net/qq_41946216/article/details/134207327
   #添加公钥到authorized_keys 
     cat ~/.ssh/id_rsa.pub >>  ~/.ssh/authorized_keys

   #06虚拟机操作工具
	   Xshell、finalshell_install、MobaXterm_Portable_v22.1
	   
2、linux搭建hadoop与spark环境
   #01jdk安装及环境变量配置
   #001 上传jdk-8u151-linux-x64.tar.gz包到 /opt/module
   #002 解压： tar -zxvf jdk-8u151-linux-x64.tar.gz
   #003 配置环境变量
       vi ~/.bash_profile (或 /etc/profile 或 ~/.profile  
       或 ~/.bashrc 或 /etc/bashrc 或 /etc/bash.bashrc[在ubuntu 中的位置]) 
       编辑
	   vi ~/.bash_profile
	   #JAVA_HOME
       export JAVA_HOME=/opt/module/java
       export JRE_HOME=/opt/module/java
       export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
       export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
   #004 使配置生效
       source ~/.bash_profile
   #005 java -version      

   #02hadoop安装及环境变量配置
   #001 上传包到hadoop-3.2.2.tar.gz包到 /opt/module
   #002 解压： tar -zxvf hadoop-3.2.2.tar.gz
   #003 配置环境变量
       vi ~/.bash_profile (或 /etc/profile 或 ~/.profile  
       或 ~/.bashrc 或 /etc/bashrc 或 /etc/bash.bashrc[在ubuntu 中的位置]) 
       编辑
	   vi ~/.bash_profile
	   #JAVA_HOME
       export HADOOP_HOME=/opt/module/hadoop
       export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   #004 使配置生效
       source ~/.bash_profile
   #005 hadoop version    


   #03spark安装及环境变量配置
   #001 上传包到spark-3.0.0-bin-hadoop3.2.tgz包到 /opt/module
   #002 解压： tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz
   #003 配置环境变量
       vi ~/.bash_profile (或 /etc/profile 或 ~/.profile  
       或 ~/.bashrc 或 /etc/bashrc 或 /etc/bash.bashrc[在ubuntu 中的位置]) 
       编辑
	   vi ~/.bash_profile
	   #SPARK_HOME
       export SPARK_HOME=/opt/module/hadoop
       export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
   #004 使配置生效
       source ~/.bash_profile
   #005 pyspark 
3、启动hdfs
   #配置hadoop
   core-site.xml，hadoop-env.sh，hdfs-site.xml，mapred-site.xml，yarn-site.xml五个配置文件覆盖到/opt/module/hadoop/etc/hadoop/中
   #格式化hdfs文件系统
   cd /opt/module/hadoop/
   bin/hdfs namenode -format
   find /opt/module/hadoop/data
   #启动hdfs分布式文件系统
   cd /opt/module/hadoop
   ./sbin/start-dfs.sh
   #使用jps查看启动进程
   jps
   #关闭防火墙
   systemctl stop firewalld    #停止Firewalld服务。
   systemctl disable firewalld  #禁止Firewalld在系统启动时自动启动。
   #访问hadoop分布式文件系统
   http://192.168.245.1:9870
# 系统注册jdk
sudo update-alternatives --install /usr/bin/java java /opt/module/jdk1.8.0_151/bin/java 300
```

spark配置：https://blog.csdn.net/qq_42881421/article/details/88069211

### 二、HDFS的基本原理

##### hadoop 1.x:

​	HDFS			负责数据存储

​	Mapreduce 负责数据计算的框架+负责资源调度和分配

##### hadoop 2.x:

​	HDFS			负责数据存储

​	Mapreduce 负责数据计算的框架

​	Yarn			 负责资源调度和分配

*HDFS是分布式系统，用来存储数据*

##### NameNode

#负责记录元数据：机器的位置、数据的大小、数据的操作时间、读数据 或 写数据、命名空间

#负责和DataNode，每3s收到DataNode发送的心跳，并返回命令给DataNode

##### DataNode

#负责数据的读

#负责数据的写

#每3s发送一次心跳给NameNode

##### SecondaryNameNode

#如果超过60分钟，主动帮NameNode 和并 edits 与 fsimage

#如果未超过60分钟且edits文件>64M，主动帮NameNode 合并 edits 与 fsimage

### 三、HDFS的常用命令

### 四、Mapreduce原理及编程